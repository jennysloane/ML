---
title: "regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

# Questions I had before/during reading:

1. What is the difference between correlation and linear regression? when should you use one vs the other?
    - pearson correlation is equivalent to a linear regression with 1 predictor
    - linear regression can have multiple predictors 
2. Is mean centering the same as standardizing coefficients? when should you do either?
  - standardizing - subtracting the mean of the variable and dividing by sd. helps with the interpretation of the estimated coefficients. puts predictors on the same scale to make them easier to compare
  - centering - just subtracting the mean. helps with the interpretation of the estimated constant/intercept 
3. what are people's opinions of $R^{2}$ vs adjusted $R^{2}$?    
  - $R^{2}$ is more interpretable whereas adjusted accounts for bias 
  - In her book, Dani says she prefers $R^{2}$
4. Are some assumptions more important than others? do people actually check the assumptions?
  - assumptions: linearity, homoscedasticity (variance of residuals is the same for any value of x), independence (observations are independent of each other), normality
  - ??
5. What does is mean to say "residuals that are independent of each other"?
  - ??
6. What are the different types of residuals? 
  - regular vs standardised vs studentised
  - ??
7. What is an ideal workflow for checking your model and all the assumptions?
  - go through example below
8. What to do when specific assumptions are violated?
  - if data is non-linear, can try a transformation and check residual vs predicted/fitted values to see if that helped
    - what are some common transformations? $var^{2}$ or log()
  - if collinearity, drop one of the problematic variables or combine variables into a single predictor

- [datasets in R](https://machinelearningmastery.com/machine-learning-datasets-in-r/)
```{r}
library(tidyverse)
library(janitor)
library(car)
library(mlbench)
library(broom)

theme_set(theme_bw())
```

# simple linear regression
- Boston Housing dataset 
- first, let's look at relationships between different variables
```{r}
data(BostonHousing)
# ?BostonHousing

boston <- BostonHousing %>%
  as_tibble() 

pairs(boston, pch=19, lower.panel = NULL)
```

- pretty clear relationship between rm (avg number of rooms per dwelling) and medv (median value of owner-occupied homes)

- dis (distance to 5 employment centers) and nox (nitric oxides concentration) perhaps quadratic relationship??
  - this seems more interesting so let's look at this 
  - also good exercise because a lot of times our data won't be linear
  
- monotonically decreasing. exponential probably better than quadratric  
```{r}
ggplot(boston, aes(dis, nox)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) # not surprising, but not a very good fit 

m1 <- lm(dis ~ nox, data=boston)
m1
summary(m1)
tidy(m1)
glance(m1)

names(m1)

m1$coefficients # or coef(m1)


predict(m1, data.frame(nox=c(.3)),
        interval="confidence") 

```

- predict function: what's it actually doing? what is fit? why does it only accept y var?
- y = mx + c
- using our model:
  - y = -13.98*0.3 + 11.55 
  - y = 7.36
- predict() also returns 7.36
- but when we look at our geom_smooth plot, it looks like nox = 0.3 should be dis = ~10 (just under). why?

# THIS IS WHERE WE'LL PICK UP NEXT WEEK :) 

# check assumptions: what to look for in each?
- plot 1: residuals and fitted values (want to see a straight line?)
- plot 2: normal QQ
- plot 3: scale location
- plot 4: residuals vs leverage
```{r}
plot(m1)

which.max(hatvalues(m1)) # which observation has the largest leverage
# what about 354? is that just an outlier?
```

# multiple linear regression
```{r}
m2 <- lm(dis ~ ., data=boston) # all predictors
summary(m2)

summary(m2)$r.sq

vif(m2) # help determine if predictors are too highly correlated
# some numbers are large what to do next? try to remove collinearity?

m3 <- lm(dis ~ . - tax - lstat, data=boston) # not sure which variables to remove? systematic way to do this?
vif(m3) 
```

# non linear transformation
- when transforming predictors so it's ^2, you need to wrap it in `I()` because ^ has a special meaning in a formula 
- the fact that the transformed predictor (quadratic term) is significant, near 0, suggests it leads to an improved model
- can do anova() to quantify the extent to which the quadratic fit is superior to the linear fit
- can use the poly() to test multiple polynomial terms at once
```{r}
m4 = lm(dis ~ nox + nox^2, data=boston)
summary(m4)

m5 = lm(dis ~ nox + I(nox^2), data=boston)
summary(m5)

anova(m1, m5)

plot(m5)

# what about ^3? or ^4
m6 = lm(dis ~ poly(nox,5), data=boston)
summary(m6)

```


