---
title: "classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Questions before/during reading:

1. maths behind logistic function? 

2. what is log-odds/logit? 
  - "logistic regression model, increasing X by one unit changes the log odds by $\beta_1$ or equivalently multiplies the odds by $e^{\beta_1}$"
  - "a one-uni increase in balance is associated withan increase in the log odds of default by 0.0055 units" (p.134)... what does that mean?

3. "The estimated intercept in Table 4.1 is typically not of interest" (p.134)...why?

4. maximum likelihood vs least squares?

```{r}
library(tidyverse)
library(mlbench)
library(tidyverse)
library(janitor)
library(caTools)
library(psych)
library(devtools)
library(MASS)
library(klaR)
library(ggord)
library(GGally)
library(ggeasy)
# install_github("fawda123/ggord")
```

# simulated data 
- Question: will students pass or fail?
- 1 predictor: hours of study
- Bayes decision boundary (in real life can't calculate the Bayes classifier, but can here because we know the distributions and parameters)
- LDA
  - $\pi_{k}$ = prior probability that an observation belongs to *k*th class
  - $\mu_{k}$ = average of all the training observations from the *k*th class
  - $\hat\sigma^2$ = weighted average of the sample variances for each of the *K* classes
```{r}
set.seed(22)

n = 100

mu_pass = 8.5
sd_pass = 1

mu_fail = 5
sd_fail = 1

pass <- rnorm(n, mean=mu_pass, sd=sd_pass) %>%
  as_tibble() %>%
  rename(study_hours = value) %>%
  mutate(pass = 1)

fail <- rnorm(n, mean=mu_fail, sd=sd_fail) %>%
  as_tibble() %>%
  rename(study_hours = value) %>%
  mutate(pass = 0)

df <- rbind(pass, fail) %>%
  mutate(id = row_number()) %>%
  relocate(id)

df$pass <- as.factor(df$pass)

# Bayes decision boundary:
bayes_bound <- (mu_pass+mu_fail)/2

```

## two one dimensional normal density functions 
- similar to Figure 4.4 left 
- Bayes decision boundary as the dotted line
```{r}
ggplot(df, aes(study_hours, color=pass)) +
  geom_density() +
  geom_vline(xintercept = bayes_bound, linetype="dashed") +
  theme_bw() +
  labs(x = "Study Hours") 
```

## 30 observations.. is this just to represent training?
- similar to Figure 4.4 right
- again with Bayes decision boundary as dashed line
```{r}
# take 30 random observations from both pass and fail classes
sample30 <- df %>%
  group_by(pass) %>%
  sample_n(30)

# estimated decision boundary (LDA?)
est <- sample30 %>%
  group_by(pass) %>%
  summarise(mean = mean(study_hours), 
            sd = sd(study_hours))

est$pass <- factor(est$pass, labels = c("Fail", "Pass"))

mu_est_pass <- est$mean[est$pass == "Pass"]
mu_est_fail <- est$mean[est$pass == "Fail"]

est_bound <- (mu_est_pass+mu_est_fail)/2

sample30$pass <- factor(sample30$pass, labels = c("Fail", "Pass"))

ggplot(sample30, aes(study_hours, fill=pass)) +
  geom_histogram(bins = 15, alpha=.7) +
  geom_vline(xintercept = bayes_bound, linetype="dashed", size = 1) +
  geom_vline(xintercept = est_bound, size = 1) + 
  labs(x = "Study Hours") +
  theme_bw() +
  easy_remove_legend_title() +
  easy_text_size(15) +
  easy_move_legend("top")
  
```

# test new observation
- picking new observations that weren't used in training, and plugging into equation 4.17 where we assign an observation (x) to the class for which the output of the equation is largest (test for both classes)
- `high_test_obs` expect larger number to be for "Pass"
- `low_test_obs` expect larger number to be for "Fail"
- `mid_test_obs` expect numbers for each classifier to be very close
```{r}
train_id <- sample30$id

df %>%
  arrange(desc(study_hours))

46 %in% train_id # just check that it's not in the training data

high_test_obs <- df %>% # 11.8 study hours
  filter(id == 46) 

df %>%
  arrange(study_hours)

low_test_obs <- df %>% # 2.5 study hours
  filter(id == 115) 

115 %in% train_id

mid_test_obs <- df %>% # near decision boundary
  filter(id == 154) 

154 %in% train_id

tabyl(sample30$pass)
est

# set which observation to test 
test_obs = high_test_obs

# fail 
x = test_obs$study_hours # new test observation
pi = .5 # prior probability that an observation belongs to kth class
mu = est$mean[est$pass == "Fail"] # average of all the training observations from the kth class
sigma = est$sd[est$pass == "Fail"]*0.5 + est$sd[est$pass == "Pass"]*0.5

x*(mu/sigma)-(mu^2/(2*sigma)) + log(pi)

# pass
x = test_obs$study_hours # new test observation
pi = .5 # prior probability that an observation belongs to kth class
mu = est$mean[est$pass == "Pass"] # average of all the training observations from the kth class
sigma = est$sd[est$pass == "Fail"]*0.5 + est$sd[est$pass == "Pass"]*0.5

x*(mu/sigma)-(mu^2/(2*sigma)) + log(pi)

```



# LDA with iris data
[Linear Discriminant Analysis in R | Example with Classification Model & Bi-Plot interpretation](https://www.youtube.com/watch?v=WUCnHx0QDSI)
- estimate relationship between a single categorical DV and a set of quantitative IVs

## model  
```{r}
set.seed(555)

mydat <- as_tibble(iris)
pairs.panels(mydat[1:4],
             bg=c("red","yellow","blue")[mydat$Species],
             pch=21)

ind <- sample(2, nrow(mydat),
              replace = TRUE, 
              prob = c(.7, .3))

training <- iris[ind==1,]
testing <- iris[ind==2,]

m1 <- lda(Species ~ ., training)
m1

m1$prior
m1$counts

```

## histograms
```{r}
pred <- predict(m1, training)

pred$class
pred$posterior
pred$x

ldahist(data = pred$x[,1], g = training$Species) # LD1
ldahist(data = pred$x[,2], g = training$Species) # LD2

ggord(m1, training$Species, ylim= c(-10,10))

```

## partition plot
- linear vs quadratic 
```{r}
partimat(Species ~., data = training, method = "lda") 
partimat(Species ~., data = training, method = "qda")
```

## Confusion Matrix - training data
```{r}
p1 <- predict(m1, training)$class

conf_mat <- table(Predicted = p1, Actual = training$Species)

acc <- sum(diag(conf_mat))/sum(conf_mat)
acc 
```

## Confusion Matrix - testing data
```{r}
p2 <- predict(m1, testing)$class

conf_mat_test <- table(Predicted = p2, Actual = testing$Species)

acc <- sum(diag(conf_mat_test))/sum(conf_mat_test)
acc
```


# Breast Cancer classification problem
```{r}

data(BreastCancer)

bc_data_raw <- BreastCancer %>%
  as_tibble() %>%
  clean_names()

str(bc_data_raw)

sum(is.na(bc_data_raw))

# going to reduce data set down to 4 variables of interest
bc_data <- bc_data_raw %>%
  na.omit() %>%
  select(cl_thickness, cell_size, cell_shape, marg_adhesion, class)

str(bc_data)

# let's reduce the number of factor levels from 10 to 3
table(bc_data$cl_thickness) 
table(bc_data$cell_size) 
table(bc_data$cell_shape) 
table(bc_data$marg_adhesion) 

bc_data_clean <- bc_data %>%
  mutate(across(cl_thickness:marg_adhesion, ~ case_when(.x %in% c(1,2,3) ~ "Low", 
                                   .x %in% c(4,5,6,7) ~ "Medium",
                                   .x %in% c(8,9,10) ~ "High"))) %>%
  na.omit()

str(bc_data_clean)

bc_data_clean$cl_thickness <- factor(bc_data_clean$cl_thickness, levels = c("Low", "Medium", "High"))
bc_data_clean$cell_size <- factor(bc_data_clean$cell_size, levels = c("Low", "Medium", "High"))
bc_data_clean$cell_shape <- factor(bc_data_clean$cell_shape, levels = c("Low", "Medium", "High"))
bc_data_clean$marg_adhesion <- factor(bc_data_clean$marg_adhesion, levels = c("Low", "Medium", "High"))
```


# Data visualisation 
```{r}
ggplot(bc_data_clean, aes(cl_thickness)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(cell_size)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(cell_shape)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(marg_adhesion)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()


```

# Split into training and testing data
```{r}
bc_data_clean$split <- sample.split(bc_data_clean$class, SplitRatio = 0.7)

bc_train <- bc_data_clean %>%
  filter(split == TRUE)

bc_test <- bc_data_clean %>%
  filter(split == FALSE)

bc_model <- glm(class ~ ., family = binomial(link = "logit"), data = bc_train)
summary(bc_model)

bc_test$predicted_class <- predict(bc_model, newdata = bc_test, type = "response") # not sure what this warning is?? 

# confusion matrix
confusion_mat <- table(bc_test$class, bc_test$predicted_class > 0.5) # compares actual class to predicted class 
confusion_mat

acc <- (confusion_mat[1,1]+confusion_mat[2,2])/sum(confusion_mat)
# model is 96% accurate
```

