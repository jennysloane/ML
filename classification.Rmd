---
title: "classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Questions before/during reading:

1. what is log-odds/logit? 
  - "logistic regression model, increasing X by one unit changes the log odds by $\beta_1$ or equivalently multiplies the odds by $e^{\beta_1}$"
  - "a one-uni increase in balance is associated withan increase in the log odds of default by 0.0055 units" (p.134)... what does that mean?

2. "The estimated intercept in Table 4.1 is typically not of interest" (p.134)...why?

3. maximum likelihood vs least squares?

4. when to use LDA?
  - more than 2 classes... can you not do a logistic regression with more than 2 classes?

5. what does an unstable model mean?

```{r}
library(tidyverse)
library(mlbench)
library(tidyverse)
library(janitor)
library(caTools)
library(psych)
library(devtools)
library(MASS)
library(klaR)
library(ggord)
library(GGally)
library(ggeasy)
# install_github("fawda123/ggord")
```

# simulated data 
- Question: will students pass or fail?
- 1 predictor: hours of study
- Bayes decision boundary (in real life can't calculate the Bayes classifier, but can here because we know the distributions and parameters)
- LDA
  - $\pi_{k}$ = prior probability that an observation belongs to *k*th class
  - $\mu_{k}$ = average of all the training observations from the *k*th class
  - $\hat\sigma^2$ = weighted average of the sample variances for each of the *K* classes
```{r}
set.seed(15)

n = 100

mu_pass = 8
sd_pass = 1

mu_fail = 5
sd_fail = 1

pass <- rnorm(n, mean=mu_pass, sd=sd_pass) %>%
  as_tibble() %>%
  rename(study_hours = value) %>%
  mutate(pass = 1)

fail <- rnorm(n, mean=mu_fail, sd=sd_fail) %>%
  as_tibble() %>%
  rename(study_hours = value) %>%
  mutate(pass = 0)

df <- rbind(pass, fail) %>%
  mutate(id = row_number()) %>%
  relocate(id)

df$pass <- as.factor(df$pass)

# Bayes decision boundary:
bayes_bound <- (mu_pass+mu_fail)/2

```

## two one dimensional normal density functions 
- similar to Figure 4.4 left 
- Bayes decision boundary as the dotted line
```{r}
ggplot(df, aes(study_hours, color=pass)) +
  geom_density() +
  geom_vline(xintercept = bayes_bound, linetype="dashed") +
  theme_bw() +
  labs(x = "Study Hours") 
```

## 30 observations.. is this just to represent training?
- similar to Figure 4.4 right
- again with Bayes decision boundary as dashed line
```{r}
# take 30 random observations from both pass and fail classes
sample30 <- df %>%
  group_by(pass) %>%
  sample_n(30)

# estimated decision boundary (LDA?)
est <- sample30 %>%
  group_by(pass) %>%
  summarise(mean = mean(study_hours), 
            sd = sd(study_hours))

est$pass <- factor(est$pass, labels = c("Fail", "Pass"))

mu_est_pass <- est$mean[est$pass == "Pass"]
mu_est_fail <- est$mean[est$pass == "Fail"]

est_bound <- (mu_est_pass+mu_est_fail)/2

sample30$pass <- factor(sample30$pass, labels = c("Fail", "Pass"))

ggplot(sample30, aes(study_hours, fill=pass)) +
  geom_histogram(bins = 15, alpha=.7) +
  geom_vline(xintercept = bayes_bound, linetype="dashed", size = 1) +
  geom_vline(xintercept = est_bound, size = 1) + 
  labs(x = "Study Hours") +
  theme_bw() +
  easy_remove_legend_title() +
  easy_text_size(15) +
  easy_move_legend("top")
  
```

# test new observation
- picking new observations that weren't used in training, and plugging into equation 4.17 where we assign an observation (x) to the class for which the output of the equation is largest (test for both classes)
- `high_test_obs` expect larger number to be for "Pass"
- `low_test_obs` expect larger number to be for "Fail"
- `mid_test_obs` expect numbers for each classifier to be very close
```{r}
train_id <- sample30$id

testing <- df %>%
  filter(!(id %in% train_id)) %>% # observations that were not part of the training data 
  arrange(study_hours) %>%
  mutate(position = row_number())

length_test = testing$position

low_test_obs <- testing %>%
  filter(position == min(length_test)) # study hours = 2.41

mid_test_obs <- testing %>%
  filter(position == max(length_test)/2) # study hours = 6.47

high_test_obs <- testing %>%
  filter(position == max(length_test)) # study hours = 10.5

tabyl(sample30$pass)
est

pass_fail_fun <- function(test_data) {
  
  # fail 
  x = test_data$study_hours # new test observation
  pi = .5 # prior probability that an observation belongs to kth class
  mu = est$mean[est$pass == "Fail"] # average of all the training observations from the kth class
  sigma = est$sd[est$pass == "Fail"]*0.5 + est$sd[est$pass == "Pass"]*0.5
  
  lda_class_fail = x*(mu/sigma)-(mu^2/(2*sigma)) + log(pi)
  
  # pass
  x = test_data$study_hours # new test observation
  pi = .5 # prior probability that an observation belongs to kth class
  mu = est$mean[est$pass == "Pass"] # average of all the training observations from the kth class
  sigma = est$sd[est$pass == "Fail"]*0.5 + est$sd[est$pass == "Pass"]*0.5
  
  lda_class_pass = x*(mu/sigma)-(mu^2/(2*sigma)) + log(pi)
  
  if(lda_class_fail > lda_class_pass) {
    output_class = "Fail"
  } else if(lda_class_pass > lda_class_fail) {
    output_class = "Pass"
  } else {
    output_class = "Undecided" # this probably won't happen
  }
  
  return(list(lda_class_fail, lda_class_pass, output_class))
  
}

# list: 
# 1) lda equation output for FAIL
# 2) lda equation output for PASS
# 3) whether the model classifies the observation as "Pass" or "Fail
pass_fail_fun(test_data = low_test_obs)

```



# LDA with iris data
[Linear Discriminant Analysis in R | Example with Classification Model & Bi-Plot interpretation](https://www.youtube.com/watch?v=WUCnHx0QDSI)
- estimate relationship between a single categorical DV and a set of quantitative IVs

## model  
```{r}
set.seed(555)

mydat <- as_tibble(iris)
pairs.panels(mydat[1:4],
             bg=c("red","yellow","blue")[mydat$Species],
             pch=21)

ind <- sample(2, nrow(mydat),
              replace = TRUE, 
              prob = c(.7, .3))

training <- iris[ind==1,]
testing <- iris[ind==2,]

m1 <- lda(Species ~ ., training)
m1

m1$prior
m1$counts

```

## histograms
```{r}
pred <- predict(m1, training)

pred$class
pred$posterior
pred$x

ldahist(data = pred$x[,1], g = training$Species) # LD1
ldahist(data = pred$x[,2], g = training$Species) # LD2

ggord(m1, training$Species, ylim= c(-10,10))

```

## partition plot
- linear vs quadratic 
```{r}
partimat(Species ~., data = training, method = "lda") 
partimat(Species ~., data = training, method = "qda")
```

## Confusion Matrix - training data
```{r}
p1 <- predict(m1, training)$class

conf_mat <- table(Predicted = p1, Actual = training$Species)

acc <- sum(diag(conf_mat))/sum(conf_mat)
acc 
```

## Confusion Matrix - testing data
```{r}
p2 <- predict(m1, testing)$class

conf_mat_test <- table(Predicted = p2, Actual = testing$Species)

acc <- sum(diag(conf_mat_test))/sum(conf_mat_test)
acc
```


# Breast Cancer classification problem
```{r}

data(BreastCancer)

bc_data_raw <- BreastCancer %>%
  as_tibble() %>%
  clean_names()

str(bc_data_raw)

sum(is.na(bc_data_raw))

# going to reduce data set down to 4 variables of interest
bc_data <- bc_data_raw %>%
  na.omit() %>%
  select(cl_thickness, cell_size, cell_shape, marg_adhesion, class)

str(bc_data)

# let's reduce the number of factor levels from 10 to 3
table(bc_data$cl_thickness) 
table(bc_data$cell_size) 
table(bc_data$cell_shape) 
table(bc_data$marg_adhesion) 

bc_data_clean <- bc_data %>%
  mutate(across(cl_thickness:marg_adhesion, ~ case_when(.x %in% c(1,2,3) ~ "Low", 
                                   .x %in% c(4,5,6,7) ~ "Medium",
                                   .x %in% c(8,9,10) ~ "High"))) %>%
  na.omit()

str(bc_data_clean)

bc_data_clean$cl_thickness <- factor(bc_data_clean$cl_thickness, levels = c("Low", "Medium", "High"))
bc_data_clean$cell_size <- factor(bc_data_clean$cell_size, levels = c("Low", "Medium", "High"))
bc_data_clean$cell_shape <- factor(bc_data_clean$cell_shape, levels = c("Low", "Medium", "High"))
bc_data_clean$marg_adhesion <- factor(bc_data_clean$marg_adhesion, levels = c("Low", "Medium", "High"))
```


# Data visualisation 
```{r}
ggplot(bc_data_clean, aes(cl_thickness)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(cell_size)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(cell_shape)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()

ggplot(bc_data_clean, aes(marg_adhesion)) +
  geom_bar(aes(fill=class), color="black") +
  theme_bw()


```

# Split into training and testing data
```{r}
bc_data_clean$split <- sample.split(bc_data_clean$class, SplitRatio = 0.7)

bc_train <- bc_data_clean %>%
  filter(split == TRUE)

bc_test <- bc_data_clean %>%
  filter(split == FALSE)

bc_model <- glm(class ~ ., family = binomial(link = "logit"), data = bc_train)
summary(bc_model)

bc_test$predicted_class <- predict(bc_model, newdata = bc_test, type = "response") # not sure what this warning is?? 

# confusion matrix
confusion_mat <- table(bc_test$class, bc_test$predicted_class > 0.5) # compares actual class to predicted class 
confusion_mat

acc <- (confusion_mat[1,1]+confusion_mat[2,2])/sum(confusion_mat)
# model is 96% accurate
```

